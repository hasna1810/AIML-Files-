{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hasna1810/AIML-Files-/blob/main/DNN_forest_cover_data_(dropouts%2Bbatchnorm).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX18pnxjJczE"
      },
      "source": [
        "\n",
        "\n",
        "# Step-1: Loading libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggFLLgLqJWY3"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# Multilayer Perceptron using Keras - forest cover Dataset\n",
        "\n",
        "import  numpy  as  np\n",
        "import  pandas  as  pd\n",
        "import  matplotlib.pyplot  as  plt\n",
        "import  tensorflow  as  tf\n",
        "from  tensorflow  import  keras\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F_DBqFi5dMjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKeKZe2lJokZ"
      },
      "source": [
        "## Step-2 Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnIV_1kLJsgQ"
      },
      "source": [
        "# Load the dataset from sklearn\n",
        "from  sklearn.datasets  import  fetch_covtype\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilmtIFViKUrh"
      },
      "source": [
        "# Step-3 Data Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQj9DijVKbW1",
        "outputId": "01d33be9-cc90-44d0-ceb3-f53bc1151825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Setting the random seeds for repeatability\n",
        "tf.set_random_seed( 42 )\n",
        "np.random.seed( 42 )\n",
        "\n",
        "# Loading the dataset\n",
        "X, y = fetch_covtype(return_X_y=True)\n",
        "\n",
        "# Split the data into 80% training and 20% testing\n",
        "from  sklearn.model_selection  import  train_test_split\n",
        "# Reduce the number of attributes, consider only first 10 attributes.\n",
        "X_10=X[:,: 10 ]\n",
        "\n",
        "\n",
        "print(X_10.shape)\n",
        "# (581012, 10)\n",
        "\n",
        "\n",
        "# Split the data into 90% training and 10% testing\n",
        "from  sklearn.model_selection  import  train_test_split\n",
        "\n",
        "\n",
        "# The 10% testing data obtained during this split will be take as our entire database.\n",
        "# This is because the original dataset is too big.\n",
        "X10_train, X10_test, y10_train, y10_test  =  train_test_split(X_10, y, test_size = 0.1 ,\n",
        "                                                            stratify = y, random_state = 42 )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://ndownloader.figshare.com/files/5976039\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(581012, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iquPsfBYJa2n"
      },
      "source": [
        "X=X10_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p88LyDuVgL2a"
      },
      "source": [
        "## Data updation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxT189j-gEk9",
        "outputId": "48bd75f3-51bc-402d-c5a2-ec92acb27632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## Data updation\n",
        "\n",
        "# Handle only the modified 1% dataset. Split that into training and testing.\n",
        "# X and y are updated with the downsized dataset\n",
        "X=X10_test\n",
        "y=y10_test\n",
        "print(set(y))\n",
        "# {1, 2, 3, 4, 5, 6, 7}\n",
        "\n",
        "# The label should start from 0, but by default, the labels are from 1 to 7.\n",
        "# Change them to the range 0 to 6 by subtracting 1 from the labels.\n",
        "y=y-1\n",
        "print(set(y))\n",
        "# {0, 1, 2, 3, 4, 5, 6}\n",
        "\n",
        "\n",
        "# Split the data into 80% training and 20% testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# While splitting, make an unbiased splitting\n",
        "X_train, X_test, y_train, y_test  =  train_test_split(X, y, test_size = 0.1 ,\n",
        "                                                            stratify = y, random_state = 42 )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1, 2, 3, 4, 5, 6, 7}\n",
            "{0, 1, 2, 3, 4, 5, 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6XwY6ZYKj_m"
      },
      "source": [
        "# Step-4  Feature Scaling- for better results on Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNKxm_MVKwYg"
      },
      "source": [
        "# Feature scaling using Standardization\n",
        "from  sklearn.preprocessing  import  StandardScaler\n",
        "sc  =  StandardScaler()\n",
        "\n",
        "# Training the feature scaling parameters\n",
        "sc.fit(X_train)\n",
        "\n",
        "# Applying transformations to both training and testing set\n",
        "X_train_std  =  sc.transform(X_train)\n",
        "\n",
        "X_test_std  =  sc.transform(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLX_L1qdK5tf"
      },
      "source": [
        "#Step 5 Neural Net Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO-t6sLbLlNb",
        "outputId": "7067112a-b860-4602-e9a5-817c2bce8b12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape[1:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5hN-kYrLFx7",
        "outputId": "ee5d0d8a-e718-4f9d-9c17-73b1cbc42f7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "source": [
        "# Create neural network using keras API\n",
        "# Sequential() does linear stacking of layers\n",
        "model_DNN  =  keras.models.Sequential()\n",
        "# Hidden layer definitions\n",
        "model_DNN.add(keras.layers.Dense(units = 25 , activation = 'relu' ,\n",
        "                                 input_shape =  X_train.shape[ 1 :]))\n",
        "# Batch Normalization is applied after the first layer transformation in this example.\n",
        "model_DNN.add(keras.layers.BatchNormalization())\n",
        "# Dropout layer with 10% of nodes being discarded in each training step.\n",
        "model_DNN.add(keras.layers.Dropout(0.1))\n",
        "#model_DNN.add(keras.layers.Dense(units = 25 , activation = 'relu' , ))\n",
        "model_DNN.add(keras.layers.Dense(units=20, activation='relu'))\n",
        "#model_DNN.add(keras.layers.Dropout(0.1))\n",
        "model_DNN.add(keras.layers.Dense(units= 15, activation='relu'))\n",
        "model_DNN.add(keras.layers.Dense(units=10, activation='relu'))\n",
        "# Output layer definitions\n",
        "model_DNN.add(keras.layers.Dense(units = 7 , activation = 'softmax' ))\n",
        "\n",
        "\n",
        "# Print the summary of network architecture\n",
        "model_DNN.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 25)                275       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 25)                100       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                520       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 15)                315       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                160       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 7)                 77        \n",
            "=================================================================\n",
            "Total params: 1,447\n",
            "Trainable params: 1,397\n",
            "Non-trainable params: 50\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N98pZTJjL5fq"
      },
      "source": [
        "# Step-6 Model Compiling Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAu5foeQL9Ia",
        "outputId": "d44dd29a-1427-4e14-94e2-3cafa692d0d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Compile the network model with relevant configurations.\n",
        "# loss, optimizer and metrics are three important configurations.\n",
        "\n",
        "model_DNN.compile(loss = 'sparse_categorical_crossentropy' , optimizer = 'adam' ,\n",
        "                  metrics = [ 'accuracy' ])\n",
        "model_DNN.fit(x = X_train_std, y = y_train, validation_split = 0.1 ,\n",
        "              epochs = 50 , batch_size = 16,verbose=False )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7eff2dc62908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX8iZ66gRjPl"
      },
      "source": [
        "#Step-7  Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtW3AMWyR5la",
        "outputId": "7e1d6677-f246-4aa7-bd64-6b8a854e6c4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "test_loss, test_accuracy  =  model_DNN.evaluate(x = X_test_std, y = y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5811/5811 [==============================] - 0s 74us/sample - loss: 0.5955 - acc: 0.7438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhX_wcwjSDHE"
      },
      "source": [
        "#Step-8 Model Accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6c_0qwsR-zo",
        "outputId": "0576e7db-fa9d-43d1-f11b-044330c41c9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The accuracy obtained can be close enough to what is obtained here.\n",
        "# In MLP loss was 0.6445 and accuracy was 0.7243\n",
        "# In DNN loss was 0.5698 and accuracy was 0.7525\n",
        "# In DNN with dropouts and batchnorm loss was 0.5955 and accuracy was 0.7437\n",
        "print (test_loss, test_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.595540766850578 0.74376184\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}